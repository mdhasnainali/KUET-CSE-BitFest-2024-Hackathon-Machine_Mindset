{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5750798722044729,
  "eval_steps": 500,
  "global_step": 180,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003194888178913738,
      "grad_norm": 1.4198819398880005,
      "learning_rate": 4e-05,
      "loss": 2.2966,
      "step": 1
    },
    {
      "epoch": 0.006389776357827476,
      "grad_norm": 1.5613313913345337,
      "learning_rate": 8e-05,
      "loss": 2.3098,
      "step": 2
    },
    {
      "epoch": 0.009584664536741214,
      "grad_norm": 1.598007321357727,
      "learning_rate": 0.00012,
      "loss": 2.3019,
      "step": 3
    },
    {
      "epoch": 0.012779552715654952,
      "grad_norm": 1.1237376928329468,
      "learning_rate": 0.00016,
      "loss": 2.1746,
      "step": 4
    },
    {
      "epoch": 0.01597444089456869,
      "grad_norm": 1.1115514039993286,
      "learning_rate": 0.0002,
      "loss": 2.0139,
      "step": 5
    },
    {
      "epoch": 0.019169329073482427,
      "grad_norm": 1.0355435609817505,
      "learning_rate": 0.00019885714285714287,
      "loss": 1.7579,
      "step": 6
    },
    {
      "epoch": 0.022364217252396165,
      "grad_norm": 1.1049916744232178,
      "learning_rate": 0.0001977142857142857,
      "loss": 1.4653,
      "step": 7
    },
    {
      "epoch": 0.025559105431309903,
      "grad_norm": 1.0231330394744873,
      "learning_rate": 0.00019657142857142858,
      "loss": 1.2227,
      "step": 8
    },
    {
      "epoch": 0.02875399361022364,
      "grad_norm": 0.8715912699699402,
      "learning_rate": 0.00019542857142857144,
      "loss": 1.0327,
      "step": 9
    },
    {
      "epoch": 0.03194888178913738,
      "grad_norm": 0.6163908243179321,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.9442,
      "step": 10
    },
    {
      "epoch": 0.03514376996805112,
      "grad_norm": 0.6440425515174866,
      "learning_rate": 0.00019314285714285717,
      "loss": 0.8145,
      "step": 11
    },
    {
      "epoch": 0.038338658146964855,
      "grad_norm": 0.5087988972663879,
      "learning_rate": 0.000192,
      "loss": 0.8841,
      "step": 12
    },
    {
      "epoch": 0.04153354632587859,
      "grad_norm": 0.577166736125946,
      "learning_rate": 0.00019085714285714287,
      "loss": 0.777,
      "step": 13
    },
    {
      "epoch": 0.04472843450479233,
      "grad_norm": 0.5702013969421387,
      "learning_rate": 0.00018971428571428573,
      "loss": 0.8679,
      "step": 14
    },
    {
      "epoch": 0.04792332268370607,
      "grad_norm": 0.5678614377975464,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.7419,
      "step": 15
    },
    {
      "epoch": 0.051118210862619806,
      "grad_norm": 0.4540432095527649,
      "learning_rate": 0.00018742857142857143,
      "loss": 0.7694,
      "step": 16
    },
    {
      "epoch": 0.054313099041533544,
      "grad_norm": 0.6342523694038391,
      "learning_rate": 0.0001862857142857143,
      "loss": 0.6641,
      "step": 17
    },
    {
      "epoch": 0.05750798722044728,
      "grad_norm": 0.5707613229751587,
      "learning_rate": 0.00018514285714285716,
      "loss": 0.7162,
      "step": 18
    },
    {
      "epoch": 0.06070287539936102,
      "grad_norm": 0.5251032710075378,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.7708,
      "step": 19
    },
    {
      "epoch": 0.06389776357827476,
      "grad_norm": 0.5668470859527588,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.6929,
      "step": 20
    },
    {
      "epoch": 0.0670926517571885,
      "grad_norm": 0.6249522566795349,
      "learning_rate": 0.00018171428571428573,
      "loss": 0.6425,
      "step": 21
    },
    {
      "epoch": 0.07028753993610223,
      "grad_norm": 0.5554564595222473,
      "learning_rate": 0.00018057142857142857,
      "loss": 0.6763,
      "step": 22
    },
    {
      "epoch": 0.07348242811501597,
      "grad_norm": 0.5839542150497437,
      "learning_rate": 0.00017942857142857143,
      "loss": 0.7296,
      "step": 23
    },
    {
      "epoch": 0.07667731629392971,
      "grad_norm": 0.5633789896965027,
      "learning_rate": 0.0001782857142857143,
      "loss": 0.7399,
      "step": 24
    },
    {
      "epoch": 0.07987220447284345,
      "grad_norm": 0.5259351134300232,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.7226,
      "step": 25
    },
    {
      "epoch": 0.08306709265175719,
      "grad_norm": 0.605650782585144,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.65,
      "step": 26
    },
    {
      "epoch": 0.08626198083067092,
      "grad_norm": 0.5920699834823608,
      "learning_rate": 0.0001748571428571429,
      "loss": 0.6515,
      "step": 27
    },
    {
      "epoch": 0.08945686900958466,
      "grad_norm": 0.4653925597667694,
      "learning_rate": 0.00017371428571428572,
      "loss": 0.6396,
      "step": 28
    },
    {
      "epoch": 0.0926517571884984,
      "grad_norm": 0.4998794198036194,
      "learning_rate": 0.0001725714285714286,
      "loss": 0.556,
      "step": 29
    },
    {
      "epoch": 0.09584664536741214,
      "grad_norm": 0.4187206029891968,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.6318,
      "step": 30
    },
    {
      "epoch": 0.09904153354632587,
      "grad_norm": 0.43902331590652466,
      "learning_rate": 0.0001702857142857143,
      "loss": 0.6291,
      "step": 31
    },
    {
      "epoch": 0.10223642172523961,
      "grad_norm": 0.40319812297821045,
      "learning_rate": 0.00016914285714285715,
      "loss": 0.6488,
      "step": 32
    },
    {
      "epoch": 0.10543130990415335,
      "grad_norm": 0.369549959897995,
      "learning_rate": 0.000168,
      "loss": 0.6865,
      "step": 33
    },
    {
      "epoch": 0.10862619808306709,
      "grad_norm": 0.4160859286785126,
      "learning_rate": 0.00016685714285714285,
      "loss": 0.6701,
      "step": 34
    },
    {
      "epoch": 0.11182108626198083,
      "grad_norm": 0.424978107213974,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.6072,
      "step": 35
    },
    {
      "epoch": 0.11501597444089456,
      "grad_norm": 0.3834036588668823,
      "learning_rate": 0.00016457142857142858,
      "loss": 0.6098,
      "step": 36
    },
    {
      "epoch": 0.1182108626198083,
      "grad_norm": 0.3230289816856384,
      "learning_rate": 0.00016342857142857145,
      "loss": 0.5232,
      "step": 37
    },
    {
      "epoch": 0.12140575079872204,
      "grad_norm": 0.3663163185119629,
      "learning_rate": 0.00016228571428571428,
      "loss": 0.643,
      "step": 38
    },
    {
      "epoch": 0.12460063897763578,
      "grad_norm": 0.39794376492500305,
      "learning_rate": 0.00016114285714285715,
      "loss": 0.5718,
      "step": 39
    },
    {
      "epoch": 0.12779552715654952,
      "grad_norm": 0.3646063506603241,
      "learning_rate": 0.00016,
      "loss": 0.6394,
      "step": 40
    },
    {
      "epoch": 0.13099041533546327,
      "grad_norm": 0.4082281291484833,
      "learning_rate": 0.00015885714285714285,
      "loss": 0.557,
      "step": 41
    },
    {
      "epoch": 0.134185303514377,
      "grad_norm": 0.38702192902565,
      "learning_rate": 0.00015771428571428571,
      "loss": 0.6241,
      "step": 42
    },
    {
      "epoch": 0.13738019169329074,
      "grad_norm": 0.413862943649292,
      "learning_rate": 0.00015657142857142858,
      "loss": 0.6913,
      "step": 43
    },
    {
      "epoch": 0.14057507987220447,
      "grad_norm": 0.3588545322418213,
      "learning_rate": 0.00015542857142857144,
      "loss": 0.6871,
      "step": 44
    },
    {
      "epoch": 0.14376996805111822,
      "grad_norm": 0.37992700934410095,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.5175,
      "step": 45
    },
    {
      "epoch": 0.14696485623003194,
      "grad_norm": 0.4028519093990326,
      "learning_rate": 0.00015314285714285714,
      "loss": 0.5624,
      "step": 46
    },
    {
      "epoch": 0.1501597444089457,
      "grad_norm": 0.43971574306488037,
      "learning_rate": 0.000152,
      "loss": 0.6215,
      "step": 47
    },
    {
      "epoch": 0.15335463258785942,
      "grad_norm": 0.4209055006504059,
      "learning_rate": 0.00015085714285714287,
      "loss": 0.5828,
      "step": 48
    },
    {
      "epoch": 0.15654952076677317,
      "grad_norm": 0.3849426805973053,
      "learning_rate": 0.0001497142857142857,
      "loss": 0.6348,
      "step": 49
    },
    {
      "epoch": 0.1597444089456869,
      "grad_norm": 0.4638165235519409,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.5057,
      "step": 50
    },
    {
      "epoch": 0.16293929712460065,
      "grad_norm": 0.4447096884250641,
      "learning_rate": 0.00014742857142857144,
      "loss": 0.6054,
      "step": 51
    },
    {
      "epoch": 0.16613418530351437,
      "grad_norm": 0.4626249372959137,
      "learning_rate": 0.0001462857142857143,
      "loss": 0.5596,
      "step": 52
    },
    {
      "epoch": 0.16932907348242812,
      "grad_norm": 0.41719475388526917,
      "learning_rate": 0.00014514285714285717,
      "loss": 0.6372,
      "step": 53
    },
    {
      "epoch": 0.17252396166134185,
      "grad_norm": 0.3775002360343933,
      "learning_rate": 0.000144,
      "loss": 0.5455,
      "step": 54
    },
    {
      "epoch": 0.1757188498402556,
      "grad_norm": 0.45969024300575256,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.639,
      "step": 55
    },
    {
      "epoch": 0.17891373801916932,
      "grad_norm": 0.358834445476532,
      "learning_rate": 0.0001417142857142857,
      "loss": 0.6156,
      "step": 56
    },
    {
      "epoch": 0.18210862619808307,
      "grad_norm": 0.42588385939598083,
      "learning_rate": 0.00014057142857142857,
      "loss": 0.6214,
      "step": 57
    },
    {
      "epoch": 0.1853035143769968,
      "grad_norm": 0.4169153571128845,
      "learning_rate": 0.00013942857142857143,
      "loss": 0.599,
      "step": 58
    },
    {
      "epoch": 0.18849840255591055,
      "grad_norm": 0.35910916328430176,
      "learning_rate": 0.0001382857142857143,
      "loss": 0.5552,
      "step": 59
    },
    {
      "epoch": 0.19169329073482427,
      "grad_norm": 0.5124635100364685,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.6663,
      "step": 60
    },
    {
      "epoch": 0.19488817891373802,
      "grad_norm": 0.3533383011817932,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.5979,
      "step": 61
    },
    {
      "epoch": 0.19808306709265175,
      "grad_norm": 0.39899107813835144,
      "learning_rate": 0.00013485714285714286,
      "loss": 0.7029,
      "step": 62
    },
    {
      "epoch": 0.2012779552715655,
      "grad_norm": 0.35343044996261597,
      "learning_rate": 0.00013371428571428573,
      "loss": 0.5657,
      "step": 63
    },
    {
      "epoch": 0.20447284345047922,
      "grad_norm": 0.39451223611831665,
      "learning_rate": 0.00013257142857142856,
      "loss": 0.5555,
      "step": 64
    },
    {
      "epoch": 0.20766773162939298,
      "grad_norm": 0.352230429649353,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.5216,
      "step": 65
    },
    {
      "epoch": 0.2108626198083067,
      "grad_norm": 0.35355061292648315,
      "learning_rate": 0.0001302857142857143,
      "loss": 0.6113,
      "step": 66
    },
    {
      "epoch": 0.21405750798722045,
      "grad_norm": 0.4372882544994354,
      "learning_rate": 0.00012914285714285713,
      "loss": 0.6533,
      "step": 67
    },
    {
      "epoch": 0.21725239616613418,
      "grad_norm": 0.41027626395225525,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.5437,
      "step": 68
    },
    {
      "epoch": 0.22044728434504793,
      "grad_norm": 0.36971136927604675,
      "learning_rate": 0.00012685714285714286,
      "loss": 0.6222,
      "step": 69
    },
    {
      "epoch": 0.22364217252396165,
      "grad_norm": 0.4610689878463745,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.6394,
      "step": 70
    },
    {
      "epoch": 0.2268370607028754,
      "grad_norm": 0.40440940856933594,
      "learning_rate": 0.0001245714285714286,
      "loss": 0.6885,
      "step": 71
    },
    {
      "epoch": 0.23003194888178913,
      "grad_norm": 0.402118057012558,
      "learning_rate": 0.00012342857142857142,
      "loss": 0.6779,
      "step": 72
    },
    {
      "epoch": 0.23322683706070288,
      "grad_norm": 0.39658427238464355,
      "learning_rate": 0.0001222857142857143,
      "loss": 0.6008,
      "step": 73
    },
    {
      "epoch": 0.2364217252396166,
      "grad_norm": 0.3760654628276825,
      "learning_rate": 0.00012114285714285715,
      "loss": 0.5932,
      "step": 74
    },
    {
      "epoch": 0.23961661341853036,
      "grad_norm": 0.5741385817527771,
      "learning_rate": 0.00012,
      "loss": 0.6199,
      "step": 75
    },
    {
      "epoch": 0.24281150159744408,
      "grad_norm": 0.3761502802371979,
      "learning_rate": 0.00011885714285714287,
      "loss": 0.6793,
      "step": 76
    },
    {
      "epoch": 0.24600638977635783,
      "grad_norm": 0.37522903084754944,
      "learning_rate": 0.0001177142857142857,
      "loss": 0.5534,
      "step": 77
    },
    {
      "epoch": 0.24920127795527156,
      "grad_norm": 0.4105699062347412,
      "learning_rate": 0.00011657142857142858,
      "loss": 0.4938,
      "step": 78
    },
    {
      "epoch": 0.2523961661341853,
      "grad_norm": 0.3688892424106598,
      "learning_rate": 0.00011542857142857145,
      "loss": 0.5541,
      "step": 79
    },
    {
      "epoch": 0.25559105431309903,
      "grad_norm": 0.3665976822376251,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.624,
      "step": 80
    },
    {
      "epoch": 0.25878594249201275,
      "grad_norm": 0.377138614654541,
      "learning_rate": 0.00011314285714285715,
      "loss": 0.5734,
      "step": 81
    },
    {
      "epoch": 0.26198083067092653,
      "grad_norm": 0.3433629870414734,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.5593,
      "step": 82
    },
    {
      "epoch": 0.26517571884984026,
      "grad_norm": 0.3414248526096344,
      "learning_rate": 0.00011085714285714286,
      "loss": 0.5652,
      "step": 83
    },
    {
      "epoch": 0.268370607028754,
      "grad_norm": 0.3732490837574005,
      "learning_rate": 0.00010971428571428573,
      "loss": 0.4767,
      "step": 84
    },
    {
      "epoch": 0.2715654952076677,
      "grad_norm": 0.44789186120033264,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.7024,
      "step": 85
    },
    {
      "epoch": 0.2747603833865815,
      "grad_norm": 0.39157700538635254,
      "learning_rate": 0.00010742857142857143,
      "loss": 0.6637,
      "step": 86
    },
    {
      "epoch": 0.2779552715654952,
      "grad_norm": 0.38555458188056946,
      "learning_rate": 0.0001062857142857143,
      "loss": 0.575,
      "step": 87
    },
    {
      "epoch": 0.28115015974440893,
      "grad_norm": 0.36508291959762573,
      "learning_rate": 0.00010514285714285714,
      "loss": 0.5349,
      "step": 88
    },
    {
      "epoch": 0.28434504792332266,
      "grad_norm": 0.38399046659469604,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.6152,
      "step": 89
    },
    {
      "epoch": 0.28753993610223644,
      "grad_norm": 0.3478845953941345,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.475,
      "step": 90
    },
    {
      "epoch": 0.29073482428115016,
      "grad_norm": 0.3680683374404907,
      "learning_rate": 0.00010171428571428572,
      "loss": 0.5498,
      "step": 91
    },
    {
      "epoch": 0.2939297124600639,
      "grad_norm": 0.418464720249176,
      "learning_rate": 0.00010057142857142859,
      "loss": 0.5145,
      "step": 92
    },
    {
      "epoch": 0.2971246006389776,
      "grad_norm": 0.40117767453193665,
      "learning_rate": 9.942857142857144e-05,
      "loss": 0.5982,
      "step": 93
    },
    {
      "epoch": 0.3003194888178914,
      "grad_norm": 0.4644814133644104,
      "learning_rate": 9.828571428571429e-05,
      "loss": 0.6168,
      "step": 94
    },
    {
      "epoch": 0.3035143769968051,
      "grad_norm": 0.39331409335136414,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.6034,
      "step": 95
    },
    {
      "epoch": 0.30670926517571884,
      "grad_norm": 0.36298486590385437,
      "learning_rate": 9.6e-05,
      "loss": 0.5779,
      "step": 96
    },
    {
      "epoch": 0.30990415335463256,
      "grad_norm": 0.4717126190662384,
      "learning_rate": 9.485714285714287e-05,
      "loss": 0.6404,
      "step": 97
    },
    {
      "epoch": 0.31309904153354634,
      "grad_norm": 0.38028186559677124,
      "learning_rate": 9.371428571428572e-05,
      "loss": 0.5834,
      "step": 98
    },
    {
      "epoch": 0.31629392971246006,
      "grad_norm": 0.3661995530128479,
      "learning_rate": 9.257142857142858e-05,
      "loss": 0.5888,
      "step": 99
    },
    {
      "epoch": 0.3194888178913738,
      "grad_norm": 0.37273675203323364,
      "learning_rate": 9.142857142857143e-05,
      "loss": 0.6025,
      "step": 100
    },
    {
      "epoch": 0.3226837060702875,
      "grad_norm": 0.36333271861076355,
      "learning_rate": 9.028571428571428e-05,
      "loss": 0.6048,
      "step": 101
    },
    {
      "epoch": 0.3258785942492013,
      "grad_norm": 0.34943893551826477,
      "learning_rate": 8.914285714285715e-05,
      "loss": 0.5396,
      "step": 102
    },
    {
      "epoch": 0.329073482428115,
      "grad_norm": 0.41331249475479126,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.7029,
      "step": 103
    },
    {
      "epoch": 0.33226837060702874,
      "grad_norm": 0.3766874372959137,
      "learning_rate": 8.685714285714286e-05,
      "loss": 0.5495,
      "step": 104
    },
    {
      "epoch": 0.3354632587859425,
      "grad_norm": 0.38869303464889526,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.6961,
      "step": 105
    },
    {
      "epoch": 0.33865814696485624,
      "grad_norm": 0.41341283917427063,
      "learning_rate": 8.457142857142858e-05,
      "loss": 0.6339,
      "step": 106
    },
    {
      "epoch": 0.34185303514376997,
      "grad_norm": 0.39666831493377686,
      "learning_rate": 8.342857142857143e-05,
      "loss": 0.5778,
      "step": 107
    },
    {
      "epoch": 0.3450479233226837,
      "grad_norm": 0.3625181317329407,
      "learning_rate": 8.228571428571429e-05,
      "loss": 0.5296,
      "step": 108
    },
    {
      "epoch": 0.34824281150159747,
      "grad_norm": 0.41366007924079895,
      "learning_rate": 8.114285714285714e-05,
      "loss": 0.5525,
      "step": 109
    },
    {
      "epoch": 0.3514376996805112,
      "grad_norm": 0.3648328185081482,
      "learning_rate": 8e-05,
      "loss": 0.621,
      "step": 110
    },
    {
      "epoch": 0.3546325878594249,
      "grad_norm": 0.36159035563468933,
      "learning_rate": 7.885714285714286e-05,
      "loss": 0.6213,
      "step": 111
    },
    {
      "epoch": 0.35782747603833864,
      "grad_norm": 0.36501675844192505,
      "learning_rate": 7.771428571428572e-05,
      "loss": 0.5083,
      "step": 112
    },
    {
      "epoch": 0.3610223642172524,
      "grad_norm": 0.37608692049980164,
      "learning_rate": 7.657142857142857e-05,
      "loss": 0.6417,
      "step": 113
    },
    {
      "epoch": 0.36421725239616615,
      "grad_norm": 0.4313634932041168,
      "learning_rate": 7.542857142857144e-05,
      "loss": 0.6213,
      "step": 114
    },
    {
      "epoch": 0.36741214057507987,
      "grad_norm": 0.3597255051136017,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.6137,
      "step": 115
    },
    {
      "epoch": 0.3706070287539936,
      "grad_norm": 0.35633254051208496,
      "learning_rate": 7.314285714285715e-05,
      "loss": 0.5574,
      "step": 116
    },
    {
      "epoch": 0.3738019169329074,
      "grad_norm": 0.4254624545574188,
      "learning_rate": 7.2e-05,
      "loss": 0.7023,
      "step": 117
    },
    {
      "epoch": 0.3769968051118211,
      "grad_norm": 0.45299598574638367,
      "learning_rate": 7.085714285714285e-05,
      "loss": 0.6091,
      "step": 118
    },
    {
      "epoch": 0.3801916932907348,
      "grad_norm": 0.3629012107849121,
      "learning_rate": 6.971428571428572e-05,
      "loss": 0.5616,
      "step": 119
    },
    {
      "epoch": 0.38338658146964855,
      "grad_norm": 0.4134620726108551,
      "learning_rate": 6.857142857142858e-05,
      "loss": 0.6328,
      "step": 120
    },
    {
      "epoch": 0.3865814696485623,
      "grad_norm": 0.4124786853790283,
      "learning_rate": 6.742857142857143e-05,
      "loss": 0.5559,
      "step": 121
    },
    {
      "epoch": 0.38977635782747605,
      "grad_norm": 0.4831748604774475,
      "learning_rate": 6.628571428571428e-05,
      "loss": 0.6059,
      "step": 122
    },
    {
      "epoch": 0.3929712460063898,
      "grad_norm": 0.534002959728241,
      "learning_rate": 6.514285714285715e-05,
      "loss": 0.5789,
      "step": 123
    },
    {
      "epoch": 0.3961661341853035,
      "grad_norm": 0.3869543671607971,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.5342,
      "step": 124
    },
    {
      "epoch": 0.3993610223642173,
      "grad_norm": 0.410016268491745,
      "learning_rate": 6.285714285714286e-05,
      "loss": 0.4799,
      "step": 125
    },
    {
      "epoch": 0.402555910543131,
      "grad_norm": 0.4581020772457123,
      "learning_rate": 6.171428571428571e-05,
      "loss": 0.5678,
      "step": 126
    },
    {
      "epoch": 0.4057507987220447,
      "grad_norm": 0.41835179924964905,
      "learning_rate": 6.0571428571428576e-05,
      "loss": 0.5297,
      "step": 127
    },
    {
      "epoch": 0.40894568690095845,
      "grad_norm": 0.38294142484664917,
      "learning_rate": 5.9428571428571434e-05,
      "loss": 0.5899,
      "step": 128
    },
    {
      "epoch": 0.41214057507987223,
      "grad_norm": 0.3778662085533142,
      "learning_rate": 5.828571428571429e-05,
      "loss": 0.5356,
      "step": 129
    },
    {
      "epoch": 0.41533546325878595,
      "grad_norm": 0.38955870270729065,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.5623,
      "step": 130
    },
    {
      "epoch": 0.4185303514376997,
      "grad_norm": 0.4356003999710083,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.6016,
      "step": 131
    },
    {
      "epoch": 0.4217252396166134,
      "grad_norm": 0.39626556634902954,
      "learning_rate": 5.485714285714286e-05,
      "loss": 0.5221,
      "step": 132
    },
    {
      "epoch": 0.4249201277955272,
      "grad_norm": 0.4097679555416107,
      "learning_rate": 5.3714285714285714e-05,
      "loss": 0.6004,
      "step": 133
    },
    {
      "epoch": 0.4281150159744409,
      "grad_norm": 0.3665449619293213,
      "learning_rate": 5.257142857142857e-05,
      "loss": 0.6366,
      "step": 134
    },
    {
      "epoch": 0.43130990415335463,
      "grad_norm": 0.3224945664405823,
      "learning_rate": 5.142857142857143e-05,
      "loss": 0.506,
      "step": 135
    },
    {
      "epoch": 0.43450479233226835,
      "grad_norm": 0.41868123412132263,
      "learning_rate": 5.028571428571429e-05,
      "loss": 0.5432,
      "step": 136
    },
    {
      "epoch": 0.43769968051118213,
      "grad_norm": 0.41687676310539246,
      "learning_rate": 4.9142857142857144e-05,
      "loss": 0.5935,
      "step": 137
    },
    {
      "epoch": 0.44089456869009586,
      "grad_norm": 0.47184476256370544,
      "learning_rate": 4.8e-05,
      "loss": 0.5826,
      "step": 138
    },
    {
      "epoch": 0.4440894568690096,
      "grad_norm": 0.4315342903137207,
      "learning_rate": 4.685714285714286e-05,
      "loss": 0.5411,
      "step": 139
    },
    {
      "epoch": 0.4472843450479233,
      "grad_norm": 0.40928685665130615,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.6163,
      "step": 140
    },
    {
      "epoch": 0.4504792332268371,
      "grad_norm": 0.3793838322162628,
      "learning_rate": 4.4571428571428574e-05,
      "loss": 0.6065,
      "step": 141
    },
    {
      "epoch": 0.4536741214057508,
      "grad_norm": 0.40814658999443054,
      "learning_rate": 4.342857142857143e-05,
      "loss": 0.577,
      "step": 142
    },
    {
      "epoch": 0.45686900958466453,
      "grad_norm": 0.3569031357765198,
      "learning_rate": 4.228571428571429e-05,
      "loss": 0.5578,
      "step": 143
    },
    {
      "epoch": 0.46006389776357826,
      "grad_norm": 0.6811386942863464,
      "learning_rate": 4.1142857142857146e-05,
      "loss": 0.6201,
      "step": 144
    },
    {
      "epoch": 0.46325878594249204,
      "grad_norm": 0.39587175846099854,
      "learning_rate": 4e-05,
      "loss": 0.5946,
      "step": 145
    },
    {
      "epoch": 0.46645367412140576,
      "grad_norm": 0.4017567038536072,
      "learning_rate": 3.885714285714286e-05,
      "loss": 0.4869,
      "step": 146
    },
    {
      "epoch": 0.4696485623003195,
      "grad_norm": 0.37258633971214294,
      "learning_rate": 3.771428571428572e-05,
      "loss": 0.5825,
      "step": 147
    },
    {
      "epoch": 0.4728434504792332,
      "grad_norm": 0.4029829502105713,
      "learning_rate": 3.6571428571428576e-05,
      "loss": 0.6451,
      "step": 148
    },
    {
      "epoch": 0.476038338658147,
      "grad_norm": 0.378179132938385,
      "learning_rate": 3.5428571428571426e-05,
      "loss": 0.4984,
      "step": 149
    },
    {
      "epoch": 0.4792332268370607,
      "grad_norm": 0.39777666330337524,
      "learning_rate": 3.428571428571429e-05,
      "loss": 0.5711,
      "step": 150
    },
    {
      "epoch": 0.48242811501597443,
      "grad_norm": 0.41516998410224915,
      "learning_rate": 3.314285714285714e-05,
      "loss": 0.5274,
      "step": 151
    },
    {
      "epoch": 0.48562300319488816,
      "grad_norm": 0.3952054977416992,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.5751,
      "step": 152
    },
    {
      "epoch": 0.48881789137380194,
      "grad_norm": 0.3367459177970886,
      "learning_rate": 3.0857142857142856e-05,
      "loss": 0.5287,
      "step": 153
    },
    {
      "epoch": 0.49201277955271566,
      "grad_norm": 0.4120500385761261,
      "learning_rate": 2.9714285714285717e-05,
      "loss": 0.5123,
      "step": 154
    },
    {
      "epoch": 0.4952076677316294,
      "grad_norm": 0.4004226624965668,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.4887,
      "step": 155
    },
    {
      "epoch": 0.4984025559105431,
      "grad_norm": 0.3113195300102234,
      "learning_rate": 2.742857142857143e-05,
      "loss": 0.397,
      "step": 156
    },
    {
      "epoch": 0.5015974440894568,
      "grad_norm": 0.4835037887096405,
      "learning_rate": 2.6285714285714286e-05,
      "loss": 0.5138,
      "step": 157
    },
    {
      "epoch": 0.5047923322683706,
      "grad_norm": 0.37206339836120605,
      "learning_rate": 2.5142857142857147e-05,
      "loss": 0.5945,
      "step": 158
    },
    {
      "epoch": 0.5079872204472844,
      "grad_norm": 0.3821963369846344,
      "learning_rate": 2.4e-05,
      "loss": 0.5371,
      "step": 159
    },
    {
      "epoch": 0.5111821086261981,
      "grad_norm": 0.39354273676872253,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 0.5339,
      "step": 160
    },
    {
      "epoch": 0.5143769968051118,
      "grad_norm": 0.4040204584598541,
      "learning_rate": 2.1714285714285715e-05,
      "loss": 0.5632,
      "step": 161
    },
    {
      "epoch": 0.5175718849840255,
      "grad_norm": 0.3888404369354248,
      "learning_rate": 2.0571428571428573e-05,
      "loss": 0.55,
      "step": 162
    },
    {
      "epoch": 0.5207667731629393,
      "grad_norm": 0.38771557807922363,
      "learning_rate": 1.942857142857143e-05,
      "loss": 0.5378,
      "step": 163
    },
    {
      "epoch": 0.5239616613418531,
      "grad_norm": 0.39404380321502686,
      "learning_rate": 1.8285714285714288e-05,
      "loss": 0.5274,
      "step": 164
    },
    {
      "epoch": 0.5271565495207667,
      "grad_norm": 0.4022693336009979,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 0.5581,
      "step": 165
    },
    {
      "epoch": 0.5303514376996805,
      "grad_norm": 0.43104878067970276,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.5011,
      "step": 166
    },
    {
      "epoch": 0.5335463258785943,
      "grad_norm": 0.40235310792922974,
      "learning_rate": 1.4857142857142858e-05,
      "loss": 0.5957,
      "step": 167
    },
    {
      "epoch": 0.536741214057508,
      "grad_norm": 0.363773375749588,
      "learning_rate": 1.3714285714285716e-05,
      "loss": 0.5948,
      "step": 168
    },
    {
      "epoch": 0.5399361022364217,
      "grad_norm": 0.4445663392543793,
      "learning_rate": 1.2571428571428573e-05,
      "loss": 0.585,
      "step": 169
    },
    {
      "epoch": 0.5431309904153354,
      "grad_norm": 0.35397854447364807,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 0.499,
      "step": 170
    },
    {
      "epoch": 0.5463258785942492,
      "grad_norm": 0.3849520981311798,
      "learning_rate": 1.0285714285714286e-05,
      "loss": 0.5353,
      "step": 171
    },
    {
      "epoch": 0.549520766773163,
      "grad_norm": 0.430237740278244,
      "learning_rate": 9.142857142857144e-06,
      "loss": 0.6055,
      "step": 172
    },
    {
      "epoch": 0.5527156549520766,
      "grad_norm": 0.35389187932014465,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.5048,
      "step": 173
    },
    {
      "epoch": 0.5559105431309904,
      "grad_norm": 0.39982113242149353,
      "learning_rate": 6.857142857142858e-06,
      "loss": 0.6027,
      "step": 174
    },
    {
      "epoch": 0.5591054313099042,
      "grad_norm": 0.38297197222709656,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 0.5383,
      "step": 175
    },
    {
      "epoch": 0.5623003194888179,
      "grad_norm": 0.3554533123970032,
      "learning_rate": 4.571428571428572e-06,
      "loss": 0.5412,
      "step": 176
    },
    {
      "epoch": 0.5654952076677316,
      "grad_norm": 0.3965649902820587,
      "learning_rate": 3.428571428571429e-06,
      "loss": 0.5488,
      "step": 177
    },
    {
      "epoch": 0.5686900958466453,
      "grad_norm": 0.4040587544441223,
      "learning_rate": 2.285714285714286e-06,
      "loss": 0.5429,
      "step": 178
    },
    {
      "epoch": 0.5718849840255591,
      "grad_norm": 0.37957438826560974,
      "learning_rate": 1.142857142857143e-06,
      "loss": 0.467,
      "step": 179
    },
    {
      "epoch": 0.5750798722044729,
      "grad_norm": 0.40197041630744934,
      "learning_rate": 0.0,
      "loss": 0.5497,
      "step": 180
    }
  ],
  "logging_steps": 1,
  "max_steps": 180,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9242339167698944e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
